# Q_Learning

1. [Learning Objectives](#learning-objectives)
2. [References](#references)
3. [Tasks](#tasks)
	1. [Load the Environment](#0-load-the-environment)
	2. [Initialize Q-table](#1-initialize-q-table)
	3. [Epsilon Greedy](#2-epsilon-greedy)
	4. [Q-learning](#3-q-learning)
	5. [Play](#4-play)
4. [Author](#author)

## Learning Objectives
At the end of this project, you are expected to be able to explain to anyone, without the help of Google:

* What is a Markov Decision Process?
* What is an environment?
* What is an agent?
* What is a state?
* What is a policy function?
* What is a value function? a state-value function? an action-value function?
* What is a discount factor?
* What is the Bellman equation?
* What is epsilon greedy?
* What is Q-learning?

## Refrences

* [An introduction to Reinforcement Learning](https://www.youtube.com/watch?v=JgvyzIkgxF0 "An introduction to Reinforcement Learning")
* [Simple Reinforcement Learning: Q-learning](https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56 "Simple Reinforcement Learning: Q-learning")
* [Reinforcement Learning - Developing Intelligent Agents](https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv "Reinforcement Learning - Developing Intelligent Agents")
* [Markov Decision Processes](https://www.youtube.com/watch?v=KovN7WKI9Y0 "Markov Decision Processes")
* [Gym Documentation](https://gym.openai.com/docs/ "Gym Documentation")

## Tasks
List of tasks with brief descriptions of each task.

### [0. Load the Environment](https://github.com/BenDoschGit/holbertonschool-machine_learning/blob/main/reinforcement_learning/0x00-q_learning/0-load_env.py "0. Load the Environment")

Write a function that loads the pre-made FrozenLakeEnv evnironment from OpenAIâ€™s gym. That takes the args, desc=None, map_name=None, is_slippery=False, where: desc is either None or a list of lists containing a custom description of the map to load for the environment, map_name is either None or a string containing the pre-made map to load, and is_slippery is a boolean to determine if the ice is slippery. The function returns the environment.

---

### [1. Initialize Q-table](https://github.com/BenDoschGit/holbertonschool-machine_learning/blob/main/reinforcement_learning/0x00-q_learning/1-q_init.py "1. Initialize Q-table")

Write a function that initializes the Q-table that takes the arg env, the FrozenLakeEnv instance, and returns the Q-table as a numpy.ndarray of zeros.

---

### [2. Epsilon Greedy](https://github.com/BenDoschGit/holbertonschool-machine_learning/blob/main/reinforcement_learning/0x00-q_learning/2-epsilon_greedy.py "2. Epsilon Greedy")

Write a function that uses epsilon-greedy to determine the next action. The function takes args Q, state, and epsilon, where: Q is a numpy.ndarray containing the q-table, state is the current state, and epsilon is the epsilon to use for the calculation. The function returns the next action index.

---

### [3. Q-learning](https://github.com/BenDoschGit/holbertonschool-machine_learning/blob/main/reinforcement_learning/0x00-q_learning/3-q_learning.py "3. Q-learning")

Write the function that performs Q-learning. The function takes the arguemnts env, Q, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, and epsilon_decay=0.05, where: env is the FrozenLakeEnv instance, Q is a numpy.ndarray containing the Q-table, episodes is the total number of episodes to train over, max_steps is the maximum number of steps per episode, alpha is the learning rate, gamma is the discount rate, epsilon is the initial threshold for epsilon greedy, min_epsilon is the minimum value that epsilon should decay to, and epsilon_decay is the decay rate for updating epsilon between episodes. The function returns Q, the updated Q-table and total_rewards, a list containing the rewards per episode.

---

### [4. Play](https://github.com/BenDoschGit/holbertonschool-machine_learning/blob/main/reinforcement_learning/0x00-q_learning/4-play.py "4. Play")

Write a function that has the trained agent play an episode. The function thakes the arguments env, Q, max_steps=100, where: env is the FrozenLakeEnv instance, Q is a numpy.ndarray containing the Q-table, and max_steps is the maximum number of steps in the episode. The function returns the total rewards for the episode.

---

## Author

[Benjamin Dosch](https://github.com/BenDoschGit)
